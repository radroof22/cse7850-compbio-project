{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2 Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade py3Dmol accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForProteinFolding were not initialized from the model checkpoint at facebook/esmfold_v1 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmForProteinFolding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\", low_cpu_mem_usage=True)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_protein = \"MGAGASAEEKHSRELEKKLKEDAEKDARTVKLLLLGAGESGKSTIVKQMKIIHQDGYSLEECLEFIAIIYGNTLQSILAIVRAMTTLNIQYGDSARQDDARKLMHMADTIEEGTMPKEMSDIIQRLWKDSGIQACFERASEYQLNDSAGYYLSDLERLVTPGYVPTEQDVLRSRVKTTGIIETQFSFKDLNFRMFDVGGQRSERKKWIHCFEGVTCIIFIAALSAYDMVLVEDDEVNRMHESLHLFNSICNHRYFATTSIVLFLNKKDVFFEKIKKAHLSICFPDYDGPNTYEDAGNYIKVQFLELNMRRDVKEIYSHMTCATDTQNVKFVFDAVTDIIIKENLKDCGLF\"\n",
    "\n",
    "tokenized_input = tokenizer([test_protein], return_tensors=\"pt\", add_special_tokens=False)['input_ids']\n",
    "tokenized_input = tokenized_input.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(tokenized_input)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Structual Embedding Generations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a40e9dd79c54ef99948ebc23de57372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "INFO:root:Read 572970 sequences from ./data/uniprot/uniprot_sprot.fasta.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDAKIKAYNLTVEGVEGFVRYSRVTKQHVAAFLKELRHSKQYENVNLIHYILTDKRVDIQHLEKDLVKDFKALVESAHRMRQGHMINVKYILYQLLKKHGHGPDGPDILTVKTGSKGVLYDDSFRKIYTDLGWKFTPL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from proteinclip.fasta_utils import read_fasta\n",
    "fasta_data_raw = read_fasta(\"./data/uniprot/uniprot_sprot.fasta.gz\")\n",
    "fasta_data_raw[list(fasta_data_raw.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminating:  3.17% of dataset for excessive length\n"
     ]
    }
   ],
   "source": [
    "# filter out excessive length\n",
    "# officially doesn't support sequences with > 1024 residuals\n",
    "\n",
    "fasta_data = {k: v for k, v in fasta_data_raw.items() if len(v) <= 1024}\n",
    "print(f\"Eliminating: {100*(1 - len(fasta_data) / len(fasta_data_raw)) : .2f}% of dataset for excessive length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_ids = list(fasta_data.keys())\n",
    "sequences = list(fasta_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForProteinFolding were not initialized from the model checkpoint at facebook/esmfold_v1 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmForProteinFolding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\", low_cpu_mem_usage=True)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  16690 MiB |  16690 MiB |  16690 MiB |    512 B   |\n",
      "|       from large pool |  16686 MiB |  16686 MiB |  16686 MiB |      0 B   |\n",
      "|       from small pool |      4 MiB |      4 MiB |      4 MiB |    512 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  16690 MiB |  16690 MiB |  16690 MiB |    512 B   |\n",
      "|       from large pool |  16686 MiB |  16686 MiB |  16686 MiB |      0 B   |\n",
      "|       from small pool |      4 MiB |      4 MiB |      4 MiB |    512 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  16687 MiB |  16687 MiB |  16687 MiB |      8 B   |\n",
      "|       from large pool |  16683 MiB |  16683 MiB |  16683 MiB |      0 B   |\n",
      "|       from small pool |      4 MiB |      4 MiB |      4 MiB |      8 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  16710 MiB |  16710 MiB |  16710 MiB |      0 B   |\n",
      "|       from large pool |  16704 MiB |  16704 MiB |  16704 MiB |      0 B   |\n",
      "|       from small pool |      6 MiB |      6 MiB |      6 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  20227 KiB |  20476 KiB |   1175 MiB |   1156 MiB |\n",
      "|       from large pool |  18432 KiB |  18432 KiB |   1170 MiB |   1152 MiB |\n",
      "|       from small pool |   1795 KiB |   2044 KiB |      5 MiB |      4 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1471    |    1471    |    1472    |       1    |\n",
      "|       from large pool |     667    |     667    |     667    |       0    |\n",
      "|       from small pool |     804    |     804    |     805    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1471    |    1471    |    1472    |       1    |\n",
      "|       from large pool |     667    |     667    |     667    |       0    |\n",
      "|       from small pool |     804    |     804    |     805    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     432    |     432    |     432    |       0    |\n",
      "|       from large pool |     429    |     429    |     429    |       0    |\n",
      "|       from small pool |       3    |       3    |       3    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       4    |       4    |      76    |      72    |\n",
      "|       from large pool |       2    |       2    |      73    |      71    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b08eb7bcb6433fbc86becdfa0af777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'EsmForProteinFoldingOutput' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m     output = model(tokenized_input)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Convert the batch embeddings to list and pair with corresponding protein IDs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m embedding_vectors = \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m()\n\u001b[32m     21\u001b[39m embeddings_dict[\u001b[33m\"\u001b[39m\u001b[33mprotein_id\u001b[39m\u001b[33m\"\u001b[39m].extend(protein_ids)\n\u001b[32m     22\u001b[39m embeddings_dict[\u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m].extend(embedding_vectors)\n",
      "\u001b[31mAttributeError\u001b[39m: 'EsmForProteinFoldingOutput' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm.auto import tqdm \n",
    "# Define a batch size (adjust based on your available resources)\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize list to store embeddings\n",
    "embeddings_dict = {\"protein_id\": [], \"embedding\": []}\n",
    "\n",
    "for i in tqdm(range(0, len(sequences), batch_size), total=(len(sequences) // batch_size) + 1):\n",
    "    batch_protein_ids = protein_ids[i:i+batch_size]\n",
    "    batch_sequences = sequences[i:i+batch_size]\n",
    "\n",
    "    tokenized_input = tokenizer(batch_sequences, return_tensors=\"pt\", add_special_tokens=False, truncation=True, max_length=1024, padding=True)['input_ids']\n",
    "    tokenized_input = tokenized_input.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(tokenized_input)\n",
    "\n",
    "\n",
    "    # Convert the batch embeddings to list and pair with corresponding protein IDs\n",
    "    embedding_vectors = output.tolist()\n",
    "    embeddings_dict[\"protein_id\"].extend(protein_ids)\n",
    "    embeddings_dict[\"embedding\"].extend(embedding_vectors)\n",
    "\n",
    "embedding_df = pd.DataFrame(embeddings)\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "embedding_df.to_parquet(\"data/structural/esmfold_embeddings.parquet\", index=False)\n",
    "print(\"Embeddings saved to data/structural/esmfold_embeddings.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 458, 7, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.unnormalized_angles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mType:\u001b[39m        EsmForProteinFoldingOutput\n",
      "\u001b[31mString form:\u001b[39m\n",
      "EsmForProteinFoldingOutput(frames=tensor([[[[ 1.6310e-01, -7.6539e-01,  4.3753e-01,  ..., -1.7970 <...> 0.2513]]],\n",
      "           device='cuda:0'), max_predicted_aligned_error=tensor(31.7500, device='cuda:0'))\n",
      "\u001b[31mLength:\u001b[39m      23\n",
      "\u001b[31mFile:\u001b[39m        ~/.conda/envs/compbio/lib/python3.13/site-packages/transformers/models/esm/modeling_esmfold.py\n",
      "\u001b[31mDocstring:\u001b[39m  \n",
      "Output type of [`EsmForProteinFoldingOutput`].\n",
      "\n",
      "Args:\n",
      "    frames (`torch.FloatTensor`):\n",
      "        Output frames.\n",
      "    sidechain_frames (`torch.FloatTensor`):\n",
      "        Output sidechain frames.\n",
      "    unnormalized_angles (`torch.FloatTensor`):\n",
      "        Predicted unnormalized backbone and side chain torsion angles.\n",
      "    angles (`torch.FloatTensor`):\n",
      "        Predicted backbone and side chain torsion angles.\n",
      "    positions (`torch.FloatTensor`):\n",
      "        Predicted positions of the backbone and side chain atoms.\n",
      "    states (`torch.FloatTensor`):\n",
      "        Hidden states from the protein folding trunk.\n",
      "    s_s (`torch.FloatTensor`):\n",
      "        Per-residue embeddings derived by concatenating the hidden states of each layer of the ESM-2 LM stem.\n",
      "    s_z (`torch.FloatTensor`):\n",
      "        Pairwise residue embeddings.\n",
      "    distogram_logits (`torch.FloatTensor`):\n",
      "        Input logits to the distogram used to compute residue distances.\n",
      "    lm_logits (`torch.FloatTensor`):\n",
      "        Logits output by the ESM-2 protein language model stem.\n",
      "    aatype (`torch.FloatTensor`):\n",
      "        Input amino acids (AlphaFold2 indices).\n",
      "    atom14_atom_exists (`torch.FloatTensor`):\n",
      "        Whether each atom exists in the atom14 representation.\n",
      "    residx_atom14_to_atom37 (`torch.FloatTensor`):\n",
      "        Mapping between atoms in the atom14 and atom37 representations.\n",
      "    residx_atom37_to_atom14 (`torch.FloatTensor`):\n",
      "        Mapping between atoms in the atom37 and atom14 representations.\n",
      "    atom37_atom_exists (`torch.FloatTensor`):\n",
      "        Whether each atom exists in the atom37 representation.\n",
      "    residue_index (`torch.FloatTensor`):\n",
      "        The index of each residue in the protein chain. Unless internal padding tokens are used, this will just be\n",
      "        a sequence of integers from 0 to `sequence_length`.\n",
      "    lddt_head (`torch.FloatTensor`):\n",
      "        Raw outputs from the lddt head used to compute plddt.\n",
      "    plddt (`torch.FloatTensor`):\n",
      "        Per-residue confidence scores. Regions of low confidence may indicate areas where the model's prediction is\n",
      "        uncertain, or where the protein structure is disordered.\n",
      "    ptm_logits (`torch.FloatTensor`):\n",
      "        Raw logits used for computing ptm.\n",
      "    ptm (`torch.FloatTensor`):\n",
      "        TM-score output representing the model's high-level confidence in the overall structure.\n",
      "    aligned_confidence_probs (`torch.FloatTensor`):\n",
      "        Per-residue confidence scores for the aligned structure.\n",
      "    predicted_aligned_error (`torch.FloatTensor`):\n",
      "        Predicted error between the model's prediction and the ground truth.\n",
      "    max_predicted_aligned_error (`torch.FloatTensor`):\n",
      "        Per-sample maximum predicted error."
     ]
    }
   ],
   "source": [
    "output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
    "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
    "\n",
    "def convert_outputs_to_pdb(outputs):\n",
    "    final_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
    "    outputs = {k: v.to(\"cpu\").numpy() for k, v in outputs.items()}\n",
    "    final_atom_positions = final_atom_positions.cpu().numpy()\n",
    "    final_atom_mask = outputs[\"atom37_atom_exists\"]\n",
    "    pdbs = []\n",
    "    for i in range(outputs[\"aatype\"].shape[0]):\n",
    "        aa = outputs[\"aatype\"][i]\n",
    "        pred_pos = final_atom_positions[i]\n",
    "        mask = final_atom_mask[i]\n",
    "        resid = outputs[\"residue_index\"][i] + 1\n",
    "        pred = OFProtein(\n",
    "            aatype=aa,\n",
    "            atom_positions=pred_pos,\n",
    "            atom_mask=mask,\n",
    "            residue_index=resid,\n",
    "            b_factors=outputs[\"plddt\"][i],\n",
    "            chain_index=outputs[\"chain_index\"][i] if \"chain_index\" in outputs else None,\n",
    "        )\n",
    "        pdbs.append(to_pdb(pred))\n",
    "    return pdbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
