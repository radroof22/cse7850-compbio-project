{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2 Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade py3Dmol accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForProteinFolding were not initialized from the model checkpoint at facebook/esmfold_v1 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmForProteinFolding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\", low_cpu_mem_usage=True)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_protein = \"MGAGASAEEKHSRELEKKLKEDAEKDARTVKLLLLGAGESGKSTIVKQMKIIHQDGYSLEECLEFIAIIYGNTLQSILAIVRAMTTLNIQYGDSARQDDARKLMHMADTIEEGTMPKEMSDIIQRLWKDSGIQACFERASEYQLNDSAGYYLSDLERLVTPGYVPTEQDVLRSRVKTTGIIETQFSFKDLNFRMFDVGGQRSERKKWIHCFEGVTCIIFIAALSAYDMVLVEDDEVNRMHESLHLFNSICNHRYFATTSIVLFLNKKDVFFEKIKKAHLSICFPDYDGPNTYEDAGNYIKVQFLELNMRRDVKEIYSHMTCATDTQNVKFVFDAVTDIIIKENLKDCGLF\"\n",
    "\n",
    "tokenized_input = tokenizer([test_protein], return_tensors=\"pt\", add_special_tokens=False)['input_ids']\n",
    "tokenized_input = tokenized_input.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(tokenized_input)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Structual Embedding Generations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3726d3ff6046496383cbbf13bfdebd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "INFO:root:Read 572970 sequences from ./data/uniprot/uniprot_sprot.fasta.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDAKIKAYNLTVEGVEGFVRYSRVTKQHVAAFLKELRHSKQYENVNLIHYILTDKRVDIQHLEKDLVKDFKALVESAHRMRQGHMINVKYILYQLLKKHGHGPDGPDILTVKTGSKGVLYDDSFRKIYTDLGWKFTPL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from proteinclip.fasta_utils import read_fasta\n",
    "fasta_data_raw = read_fasta(\"./data/uniprot/uniprot_sprot.fasta.gz\")\n",
    "fasta_data_raw[list(fasta_data_raw.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminating:  3.17% of dataset for excessive length\n"
     ]
    }
   ],
   "source": [
    "# filter out excessive length\n",
    "# officially doesn't support sequences with > 1024 residuals\n",
    "\n",
    "fasta_data = {k: v for k, v in fasta_data_raw.items() if len(v) <= 1024}\n",
    "print(f\"Eliminating: {100*(1 - len(fasta_data) / len(fasta_data_raw)) : .2f}% of dataset for excessive length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_ids = list(fasta_data.keys())\n",
    "sequences = list(fasta_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForProteinFolding were not initialized from the model checkpoint at facebook/esmfold_v1 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmForProteinFolding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\", low_cpu_mem_usage=True)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  16690 MiB |  16690 MiB |  16690 MiB |    512 B   |\n",
      "|       from large pool |  16686 MiB |  16686 MiB |  16686 MiB |      0 B   |\n",
      "|       from small pool |      4 MiB |      4 MiB |      4 MiB |    512 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  16690 MiB |  16690 MiB |  16690 MiB |    512 B   |\n",
      "|       from large pool |  16686 MiB |  16686 MiB |  16686 MiB |      0 B   |\n",
      "|       from small pool |      4 MiB |      4 MiB |      4 MiB |    512 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  16687 MiB |  16687 MiB |  16687 MiB |      8 B   |\n",
      "|       from large pool |  16683 MiB |  16683 MiB |  16683 MiB |      0 B   |\n",
      "|       from small pool |      4 MiB |      4 MiB |      4 MiB |      8 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  16710 MiB |  16710 MiB |  16710 MiB |      0 B   |\n",
      "|       from large pool |  16704 MiB |  16704 MiB |  16704 MiB |      0 B   |\n",
      "|       from small pool |      6 MiB |      6 MiB |      6 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  20227 KiB |  20476 KiB |   1175 MiB |   1156 MiB |\n",
      "|       from large pool |  18432 KiB |  18432 KiB |   1170 MiB |   1152 MiB |\n",
      "|       from small pool |   1795 KiB |   2044 KiB |      5 MiB |      4 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1471    |    1471    |    1472    |       1    |\n",
      "|       from large pool |     667    |     667    |     667    |       0    |\n",
      "|       from small pool |     804    |     804    |     805    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1471    |    1471    |    1472    |       1    |\n",
      "|       from large pool |     667    |     667    |     667    |       0    |\n",
      "|       from small pool |     804    |     804    |     805    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     432    |     432    |     432    |       0    |\n",
      "|       from large pool |     429    |     429    |     429    |       0    |\n",
      "|       from small pool |       3    |       3    |       3    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       4    |       4    |      76    |      72    |\n",
      "|       from large pool |       2    |       2    |      73    |      71    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b08eb7bcb6433fbc86becdfa0af777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'EsmForProteinFoldingOutput' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m     output = model(tokenized_input)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Convert the batch embeddings to list and pair with corresponding protein IDs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m embedding_vectors = \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m()\n\u001b[32m     21\u001b[39m embeddings_dict[\u001b[33m\"\u001b[39m\u001b[33mprotein_id\u001b[39m\u001b[33m\"\u001b[39m].extend(protein_ids)\n\u001b[32m     22\u001b[39m embeddings_dict[\u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m].extend(embedding_vectors)\n",
      "\u001b[31mAttributeError\u001b[39m: 'EsmForProteinFoldingOutput' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm.auto import tqdm \n",
    "# Define a batch size (adjust based on your available resources)\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize list to store embeddings\n",
    "embeddings_dict = {\"protein_id\": [], \"embedding\": []}\n",
    "\n",
    "for i in tqdm(range(0, len(sequences), batch_size), total=(len(sequences) // batch_size) + 1):\n",
    "    batch_protein_ids = protein_ids[i:i+batch_size]\n",
    "    batch_sequences = sequences[i:i+batch_size]\n",
    "\n",
    "    tokenized_input = tokenizer(batch_sequences, return_tensors=\"pt\", add_special_tokens=False, truncation=True, max_length=1024, padding=True)['input_ids']\n",
    "    tokenized_input = tokenized_input.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(tokenized_input)\n",
    "\n",
    "\n",
    "    # Convert the batch embeddings to list and pair with corresponding protein IDs\n",
    "    embedding_vectors = output.tolist()\n",
    "    embeddings_dict[\"protein_id\"].extend(protein_ids)\n",
    "    embeddings_dict[\"embedding\"].extend(embedding_vectors)\n",
    "\n",
    "embedding_df = pd.DataFrame(embeddings)\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "embedding_df.to_parquet(\"data/structural/esmfold_embeddings.parquet\", index=False)\n",
    "print(\"Embeddings saved to data/structural/esmfold_embeddings.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
    "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
    "\n",
    "def convert_outputs_to_pdb(outputs):\n",
    "    final_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
    "    outputs = {k: v.to(\"cpu\").numpy() for k, v in outputs.items()}\n",
    "    final_atom_positions = final_atom_positions.cpu().numpy()\n",
    "    final_atom_mask = outputs[\"atom37_atom_exists\"]\n",
    "    pdbs = []\n",
    "    for i in range(outputs[\"aatype\"].shape[0]):\n",
    "        aa = outputs[\"aatype\"][i]\n",
    "        pred_pos = final_atom_positions[i]\n",
    "        mask = final_atom_mask[i]\n",
    "        resid = outputs[\"residue_index\"][i] + 1\n",
    "        pred = OFProtein(\n",
    "            aatype=aa,\n",
    "            atom_positions=pred_pos,\n",
    "            atom_mask=mask,\n",
    "            residue_index=resid,\n",
    "            b_factors=outputs[\"plddt\"][i],\n",
    "            chain_index=outputs[\"chain_index\"][i] if \"chain_index\" in outputs else None,\n",
    "        )\n",
    "        pdbs.append(to_pdb(pred))\n",
    "    return pdbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43moutput\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "test_protein = \"MGAGASAEEKHSRELEKKLKEDAEKDARTVKLLLLGAGESGKSTIVKQMKIIHQDGYSLEECLEFIAIIYGNTLQSILAIVRAMTTLNIQYGDSARQDDARKLMHMADTIEEGTMPKEMSDIIQRLWKDSGIQACFERASEYQLNDSAGYYLSDLERLVTPGYVPTEQDVLRSRVKTTGIIETQFSFKDLNFRMFDVGGQRSERKKWIHCFEGVTCIIFIAALSAYDMVLVEDDEVNRMHESLHLFNSICNHRYFATTSIVLFLNKKDVFFEKIKKAHLSICFPDYDGPNTYEDAGNYIKVQFLELNMRRDVKEIYSHMTCATDTQNVKFVFDAVTDIIIKENLKDCGLF\"\n",
    "protein_id = \"1\"\n",
    "output_dict = {\"protein_id\": [], \"sequence\": [], \"pdb\": []}\n",
    "\n",
    "\n",
    "tokenized_input = tokenizer([test_protein], return_tensors=\"pt\", add_special_tokens=False)['input_ids']\n",
    "tokenized_input = tokenized_input.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(tokenized_input)\n",
    "    if len(output_dict.keys()) < 4:\n",
    "        for k in output.keys():\n",
    "            output_dict[k] = []\n",
    "\n",
    "    for k in output.keys():\n",
    "        output_dict[k].append(output[k].cpu().numpy())\n",
    "\n",
    "    output_dict[\"protein_id\"].append(protein_id)\n",
    "    output_dict[\"sequence\"].append(test_protein)\n",
    "    output_dict[\"pdb\"].append(model.output_to_pdb(output))\n",
    "\n",
    "output_df = pd.DataFrame(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"structural_test.npy\", output_df.to_numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_loaded = np.load(\"structural_test.npy\", allow_pickle=True)\n",
    "output_df_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict_loaded = {k : [] for k in ['protein_id', 'sequence', 'pdb', 'frames', 'sidechain_frames',\n",
    "       'unnormalized_angles', 'angles', 'positions', 'states', 's_s', 's_z',\n",
    "       'distogram_logits', 'lm_logits', 'aatype', 'atom14_atom_exists',\n",
    "       'residx_atom14_to_atom37', 'residx_atom37_to_atom14',\n",
    "       'atom37_atom_exists', 'residue_index', 'lddt_head', 'plddt',\n",
    "       'ptm_logits', 'ptm', 'aligned_confidence_probs',\n",
    "       'predicted_aligned_error', 'max_predicted_aligned_error']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array(0.94865006, dtype=float32) (numpy-scalar) is not JSON serializable at the moment",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43moutput_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstructural_test.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/pandas/core/generic.py:2702\u001b[39m, in \u001b[36mNDFrame.to_json\u001b[39m\u001b[34m(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001b[39m\n\u001b[32m   2699\u001b[39m config.is_nonnegative_int(indent)\n\u001b[32m   2700\u001b[39m indent = indent \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2705\u001b[39m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2715\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2716\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2717\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/pandas/io/json/_json.py:210\u001b[39m, in \u001b[36mto_json\u001b[39m\u001b[34m(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mobj\u001b[39m\u001b[33m'\u001b[39m\u001b[33m should be a Series or a DataFrame\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    200\u001b[39m s = \u001b[43mwriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lines:\n\u001b[32m    213\u001b[39m     s = convert_to_line_delimits(s)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/pandas/io/json/_json.py:263\u001b[39m, in \u001b[36mWriter.write\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    262\u001b[39m     iso_dates = \u001b[38;5;28mself\u001b[39m.date_format == \u001b[33m\"\u001b[39m\u001b[33miso\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mujson_dumps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj_to_write\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43miso_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43miso_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: array(0.94865006, dtype=float32) (numpy-scalar) is not JSON serializable at the moment"
     ]
    }
   ],
   "source": [
    "output_df.to_json(\"structural_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EsmForProteinFolding(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 2560, padding_idx=1)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 2560, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-35): 36 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "              (key): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "              (value): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (rotary_embeddings): RotaryEmbedding()\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=1440, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (esm_s_mlp): Sequential(\n",
      "    (0): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=2560, out_features=1024, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (embedding): Embedding(23, 1024, padding_idx=0)\n",
      "  (trunk): EsmFoldingTrunk(\n",
      "    (pairwise_positional_embedding): EsmFoldRelativePosition(\n",
      "      (embedding): Embedding(66, 128)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-47): 48 x EsmFoldTriangularSelfAttentionBlock(\n",
      "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequence_to_pair): EsmFoldSequenceToPair(\n",
      "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (pair_to_sequence): EsmFoldPairToSequence(\n",
      "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
      "        )\n",
      "        (seq_attention): EsmFoldSelfAttention(\n",
      "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (tri_mul_out): EsmFoldTriangleMultiplicativeUpdate(\n",
      "          (linear_a_p): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_a_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_b_p): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_b_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_z): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (layer_norm_in): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (layer_norm_out): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (tri_mul_in): EsmFoldTriangleMultiplicativeUpdate(\n",
      "          (linear_a_p): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_a_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_b_p): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_b_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_z): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (layer_norm_in): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (layer_norm_out): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (tri_att_start): EsmFoldTriangleAttention(\n",
      "          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear): EsmFoldLinear(in_features=128, out_features=4, bias=False)\n",
      "          (mha): EsmFoldAttention(\n",
      "            (linear_q): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
      "            (linear_k): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
      "            (linear_v): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
      "            (linear_o): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "        (tri_att_end): EsmFoldTriangleAttention(\n",
      "          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear): EsmFoldLinear(in_features=128, out_features=4, bias=False)\n",
      "          (mha): EsmFoldAttention(\n",
      "            (linear_q): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
      "            (linear_k): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
      "            (linear_v): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
      "            (linear_o): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "        (mlp_seq): EsmFoldResidueMLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (2): ReLU()\n",
      "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (4): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp_pair): EsmFoldResidueMLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (2): ReLU()\n",
      "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (4): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "        (row_drop): EsmFoldDropout(\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (col_drop): EsmFoldDropout(\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (recycle_s_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (recycle_z_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (recycle_disto): Embedding(15, 128)\n",
      "    (structure_module): EsmFoldStructureModule(\n",
      "      (layer_norm_s): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm_z): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear_in): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
      "      (ipa): EsmFoldInvariantPointAttention(\n",
      "        (linear_q): EsmFoldLinear(in_features=384, out_features=192, bias=True)\n",
      "        (linear_kv): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
      "        (linear_q_points): EsmFoldLinear(in_features=384, out_features=144, bias=True)\n",
      "        (linear_kv_points): EsmFoldLinear(in_features=384, out_features=432, bias=True)\n",
      "        (linear_b): EsmFoldLinear(in_features=128, out_features=12, bias=True)\n",
      "        (linear_out): EsmFoldLinear(in_features=2112, out_features=384, bias=True)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "      )\n",
      "      (ipa_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm_ipa): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (transition): EsmFoldStructureModuleTransition(\n",
      "        (layers): ModuleList(\n",
      "          (0): EsmFoldStructureModuleTransitionLayer(\n",
      "            (linear_1): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
      "            (linear_2): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
      "            (linear_3): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (bb_update): EsmFoldBackboneUpdate(\n",
      "        (linear): EsmFoldLinear(in_features=384, out_features=6, bias=True)\n",
      "      )\n",
      "      (angle_resnet): EsmFoldAngleResnet(\n",
      "        (linear_in): EsmFoldLinear(in_features=384, out_features=128, bias=True)\n",
      "        (linear_initial): EsmFoldLinear(in_features=384, out_features=128, bias=True)\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x EsmFoldAngleResnetBlock(\n",
      "            (linear_1): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_2): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (linear_out): EsmFoldLinear(in_features=128, out_features=14, bias=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (trunk2sm_s): Linear(in_features=1024, out_features=384, bias=True)\n",
      "    (trunk2sm_z): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (distogram_head): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (ptm_head): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (lm_head): Linear(in_features=1024, out_features=23, bias=True)\n",
      "  (lddt_head): Sequential(\n",
      "    (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=384, out_features=128, bias=True)\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): Linear(in_features=128, out_features=1850, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
