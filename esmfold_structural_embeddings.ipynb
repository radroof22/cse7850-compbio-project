{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2 Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade py3Dmol accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForProteinFolding were not initialized from the model checkpoint at facebook/esmfold_v1 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmForProteinFolding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\", low_cpu_mem_usage=True)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_protein = \"MGAGASAEEKHSRELEKKLKEDAEKDARTVKLLLLGAGESGKSTIVKQMKIIHQDGYSLEECLEFIAIIYGNTLQSILAIVRAMTTLNIQYGDSARQDDARKLMHMADTIEEGTMPKEMSDIIQRLWKDSGIQACFERASEYQLNDSAGYYLSDLERLVTPGYVPTEQDVLRSRVKTTGIIETQFSFKDLNFRMFDVGGQRSERKKWIHCFEGVTCIIFIAALSAYDMVLVEDDEVNRMHESLHLFNSICNHRYFATTSIVLFLNKKDVFFEKIKKAHLSICFPDYDGPNTYEDAGNYIKVQFLELNMRRDVKEIYSHMTCATDTQNVKFVFDAVTDIIIKENLKDCGLF\"\n",
    "\n",
    "tokenized_input = tokenizer([test_protein], return_tensors=\"pt\", add_special_tokens=False)['input_ids']\n",
    "tokenized_input = tokenized_input.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(tokenized_input)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Structual Embedding Generations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>organism</th>\n",
       "      <th>organism_id</th>\n",
       "      <th>name</th>\n",
       "      <th>evidence</th>\n",
       "      <th>function</th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>9606</td>\n",
       "      <td>EPHA7</td>\n",
       "      <td>1</td>\n",
       "      <td>Receptor tyrosine kinase which binds promiscuo...</td>\n",
       "      <td>Q15375</td>\n",
       "      <td>[-0.015253728, 0.016237658, -0.016555615, 0.02...</td>\n",
       "      <td>MVFQTRYPSWIILCYIWLLRFAHTGEAQAAKEVLLLDSKAQQTELE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>9606</td>\n",
       "      <td>ANXA8</td>\n",
       "      <td>1</td>\n",
       "      <td>This protein is an anticoagulant protein that ...</td>\n",
       "      <td>P13928</td>\n",
       "      <td>[-0.008352073, 0.00474287, 0.006541474, -0.002...</td>\n",
       "      <td>MAWWKSWIEQEGVTVKSSSHFNPDPDAETLYKAMKGIGTNEQAIID...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>9606</td>\n",
       "      <td>DPY19L2P1</td>\n",
       "      <td>2</td>\n",
       "      <td>Probable C-mannosyltransferase that mediates C...</td>\n",
       "      <td>Q6NXN4</td>\n",
       "      <td>[-0.00039709447, -0.02393247, -0.014100584, 0....</td>\n",
       "      <td>MKKQGVNPKPLQSSRPSPSKRPYGASPARELEVEKSALGGGKLPGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>9606</td>\n",
       "      <td>NR1D1</td>\n",
       "      <td>1</td>\n",
       "      <td>Transcriptional repressor which coordinates ci...</td>\n",
       "      <td>P20393</td>\n",
       "      <td>[0.008172105, -0.0116752, -0.016805198, -0.005...</td>\n",
       "      <td>MTTLDSNNNTGGVITYIGSSGSSPSRTSPESLYSDNSNGSFQSLTQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>9606</td>\n",
       "      <td>SLC15A2</td>\n",
       "      <td>1</td>\n",
       "      <td>Proton-coupled amino-acid transporter that tra...</td>\n",
       "      <td>Q16348</td>\n",
       "      <td>[0.0031013805, -0.0019497981, -0.003831747, 0....</td>\n",
       "      <td>MNPFQKNESKETLFSPVSIEEVPPRPPSPPKKPSPTICGSNYPLSI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               organism organism_id       name  evidence  \\\n",
       "0  Homo sapiens (Human)        9606      EPHA7         1   \n",
       "1  Homo sapiens (Human)        9606      ANXA8         1   \n",
       "2  Homo sapiens (Human)        9606  DPY19L2P1         2   \n",
       "3  Homo sapiens (Human)        9606      NR1D1         1   \n",
       "4  Homo sapiens (Human)        9606    SLC15A2         1   \n",
       "\n",
       "                                            function      id  \\\n",
       "0  Receptor tyrosine kinase which binds promiscuo...  Q15375   \n",
       "1  This protein is an anticoagulant protein that ...  P13928   \n",
       "2  Probable C-mannosyltransferase that mediates C...  Q6NXN4   \n",
       "3  Transcriptional repressor which coordinates ci...  P20393   \n",
       "4  Proton-coupled amino-acid transporter that tra...  Q16348   \n",
       "\n",
       "                                           embedding  \\\n",
       "0  [-0.015253728, 0.016237658, -0.016555615, 0.02...   \n",
       "1  [-0.008352073, 0.00474287, 0.006541474, -0.002...   \n",
       "2  [-0.00039709447, -0.02393247, -0.014100584, 0....   \n",
       "3  [0.008172105, -0.0116752, -0.016805198, -0.005...   \n",
       "4  [0.0031013805, -0.0019497981, -0.003831747, 0....   \n",
       "\n",
       "                                            sequence  \n",
       "0  MVFQTRYPSWIILCYIWLLRFAHTGEAQAAKEVLLLDSKAQQTELE...  \n",
       "1  MAWWKSWIEQEGVTVKSSSHFNPDPDAETLYKAMKGIGTNEQAIID...  \n",
       "2  MKKQGVNPKPLQSSRPSPSKRPYGASPARELEVEKSALGGGKLPGG...  \n",
       "3  MTTLDSNNNTGGVITYIGSSGSSPSRTSPESLYSDNSNGSFQSLTQ...  \n",
       "4  MNPFQKNESKETLFSPVSIEEVPPRPPSPPKKPSPTICGSNYPLSI...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"./data/dataset/sequence_and_embeddings.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForProteinFolding were not initialized from the model checkpoint at facebook/esmfold_v1 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmForProteinFolding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\", low_cpu_mem_usage=True)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  13601 MiB |  13601 MiB |  13601 MiB |      0 B   |\n",
      "|       from large pool |  13469 MiB |  13469 MiB |  13469 MiB |      0 B   |\n",
      "|       from small pool |    132 MiB |    132 MiB |    132 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  13601 MiB |  13601 MiB |  13601 MiB |      0 B   |\n",
      "|       from large pool |  13469 MiB |  13469 MiB |  13469 MiB |      0 B   |\n",
      "|       from small pool |    132 MiB |    132 MiB |    132 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  13456 MiB |  13456 MiB |  13456 MiB |      0 B   |\n",
      "|       from large pool |  13324 MiB |  13324 MiB |  13324 MiB |      0 B   |\n",
      "|       from small pool |    132 MiB |    132 MiB |    132 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  13612 MiB |  13612 MiB |  13612 MiB |      0 B   |\n",
      "|       from large pool |  13478 MiB |  13478 MiB |  13478 MiB |      0 B   |\n",
      "|       from small pool |    134 MiB |    134 MiB |    134 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  10764 KiB |  20448 KiB |    906 MiB |    895 MiB |\n",
      "|       from large pool |   9120 KiB |  18412 KiB |    785 MiB |    777 MiB |\n",
      "|       from small pool |   1644 KiB |   2045 KiB |    120 MiB |    118 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    4537    |    4537    |    4537    |       0    |\n",
      "|       from large pool |     461    |     461    |     461    |       0    |\n",
      "|       from small pool |    4076    |    4076    |    4076    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    4537    |    4537    |    4537    |       0    |\n",
      "|       from large pool |     461    |     461    |     461    |       0    |\n",
      "|       from small pool |    4076    |    4076    |    4076    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     430    |     430    |     430    |       0    |\n",
      "|       from large pool |     363    |     363    |     363    |       0    |\n",
      "|       from small pool |      67    |      67    |      67    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       4    |       6    |     117    |     113    |\n",
      "|       from large pool |       1    |       2    |      50    |      49    |\n",
      "|       from small pool |       3    |       4    |      67    |      64    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>organism</th>\n",
       "      <th>organism_id</th>\n",
       "      <th>name</th>\n",
       "      <th>evidence</th>\n",
       "      <th>function</th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>9606</td>\n",
       "      <td>EPHA7</td>\n",
       "      <td>1</td>\n",
       "      <td>Receptor tyrosine kinase which binds promiscuo...</td>\n",
       "      <td>Q15375</td>\n",
       "      <td>[-0.015253728, 0.016237658, -0.016555615, 0.02...</td>\n",
       "      <td>MVFQTRYPSWIILCYIWLLRFAHTGEAQAAKEVLLLDSKAQQTELE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>9606</td>\n",
       "      <td>ANXA8</td>\n",
       "      <td>1</td>\n",
       "      <td>This protein is an anticoagulant protein that ...</td>\n",
       "      <td>P13928</td>\n",
       "      <td>[-0.008352073, 0.00474287, 0.006541474, -0.002...</td>\n",
       "      <td>MAWWKSWIEQEGVTVKSSSHFNPDPDAETLYKAMKGIGTNEQAIID...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>9606</td>\n",
       "      <td>DPY19L2P1</td>\n",
       "      <td>2</td>\n",
       "      <td>Probable C-mannosyltransferase that mediates C...</td>\n",
       "      <td>Q6NXN4</td>\n",
       "      <td>[-0.00039709447, -0.02393247, -0.014100584, 0....</td>\n",
       "      <td>MKKQGVNPKPLQSSRPSPSKRPYGASPARELEVEKSALGGGKLPGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>9606</td>\n",
       "      <td>NR1D1</td>\n",
       "      <td>1</td>\n",
       "      <td>Transcriptional repressor which coordinates ci...</td>\n",
       "      <td>P20393</td>\n",
       "      <td>[0.008172105, -0.0116752, -0.016805198, -0.005...</td>\n",
       "      <td>MTTLDSNNNTGGVITYIGSSGSSPSRTSPESLYSDNSNGSFQSLTQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>9606</td>\n",
       "      <td>SLC15A2</td>\n",
       "      <td>1</td>\n",
       "      <td>Proton-coupled amino-acid transporter that tra...</td>\n",
       "      <td>Q16348</td>\n",
       "      <td>[0.0031013805, -0.0019497981, -0.003831747, 0....</td>\n",
       "      <td>MNPFQKNESKETLFSPVSIEEVPPRPPSPPKKPSPTICGSNYPLSI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               organism organism_id       name  evidence  \\\n",
       "0  Homo sapiens (Human)        9606      EPHA7         1   \n",
       "1  Homo sapiens (Human)        9606      ANXA8         1   \n",
       "2  Homo sapiens (Human)        9606  DPY19L2P1         2   \n",
       "3  Homo sapiens (Human)        9606      NR1D1         1   \n",
       "4  Homo sapiens (Human)        9606    SLC15A2         1   \n",
       "\n",
       "                                            function      id  \\\n",
       "0  Receptor tyrosine kinase which binds promiscuo...  Q15375   \n",
       "1  This protein is an anticoagulant protein that ...  P13928   \n",
       "2  Probable C-mannosyltransferase that mediates C...  Q6NXN4   \n",
       "3  Transcriptional repressor which coordinates ci...  P20393   \n",
       "4  Proton-coupled amino-acid transporter that tra...  Q16348   \n",
       "\n",
       "                                           embedding  \\\n",
       "0  [-0.015253728, 0.016237658, -0.016555615, 0.02...   \n",
       "1  [-0.008352073, 0.00474287, 0.006541474, -0.002...   \n",
       "2  [-0.00039709447, -0.02393247, -0.014100584, 0....   \n",
       "3  [0.008172105, -0.0116752, -0.016805198, -0.005...   \n",
       "4  [0.0031013805, -0.0019497981, -0.003831747, 0....   \n",
       "\n",
       "                                            sequence  \n",
       "0  MVFQTRYPSWIILCYIWLLRFAHTGEAQAAKEVLLLDSKAQQTELE...  \n",
       "1  MAWWKSWIEQEGVTVKSSSHFNPDPDAETLYKAMKGIGTNEQAIID...  \n",
       "2  MKKQGVNPKPLQSSRPSPSKRPYGASPARELEVEKSALGGGKLPGG...  \n",
       "3  MTTLDSNNNTGGVITYIGSSGSSPSRTSPESLYSDNSNGSFQSLTQ...  \n",
       "4  MNPFQKNESKETLFSPVSIEEVPPRPPSPPKKPSPTICGSNYPLSI...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4475b9331c9547ffa3523a1515f6e75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 59.25 GiB. GPU 0 has a total capacity of 79.22 GiB of which 44.74 GiB is free. Including non-PyTorch memory, this process has 34.46 GiB memory in use. Of the allocated memory 33.53 GiB is allocated by PyTorch, and 276.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     21\u001b[39m tokenized_input = tokenizer(\n\u001b[32m     22\u001b[39m     sequences, \n\u001b[32m     23\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     max_length=\u001b[32m1024\u001b[39m\n\u001b[32m     28\u001b[39m )[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].cuda()\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Run the model on the batched input\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Initialize any additional keys from the model output during the first batch\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/transformers/models/esm/modeling_esmfold.py:2156\u001b[39m, in \u001b[36mEsmForProteinFolding.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, masking_pattern, num_recycles)\u001b[39m\n\u001b[32m   2153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.esmfold_config.embed_aa:\n\u001b[32m   2154\u001b[39m     s_s_0 += \u001b[38;5;28mself\u001b[39m.embedding(masked_aa)\n\u001b[32m-> \u001b[39m\u001b[32m2156\u001b[39m structure: \u001b[38;5;28mdict\u001b[39m = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_s_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_z_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_recycles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_recycles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2157\u001b[39m \u001b[38;5;66;03m# Documenting what we expect:\u001b[39;00m\n\u001b[32m   2158\u001b[39m structure = {\n\u001b[32m   2159\u001b[39m     k: v\n\u001b[32m   2160\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m structure.items()\n\u001b[32m   (...)\u001b[39m\u001b[32m   2171\u001b[39m     ]\n\u001b[32m   2172\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/transformers/models/esm/modeling_esmfold.py:1962\u001b[39m, in \u001b[36mEsmFoldingTrunk.forward\u001b[39m\u001b[34m(self, seq_feats, pair_feats, true_aa, residx, mask, no_recycles)\u001b[39m\n\u001b[32m   1959\u001b[39m recycle_z = \u001b[38;5;28mself\u001b[39m.recycle_z_norm(recycle_z.detach()).to(device)\n\u001b[32m   1960\u001b[39m recycle_z += \u001b[38;5;28mself\u001b[39m.recycle_disto(recycle_bins.detach()).to(device)\n\u001b[32m-> \u001b[39m\u001b[32m1962\u001b[39m s_s, s_z = \u001b[43mtrunk_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_s_0\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecycle_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_z_0\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecycle_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1964\u001b[39m \u001b[38;5;66;03m# === Structure module ===\u001b[39;00m\n\u001b[32m   1965\u001b[39m structure = \u001b[38;5;28mself\u001b[39m.structure_module(\n\u001b[32m   1966\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33msingle\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.trunk2sm_s(s_s), \u001b[33m\"\u001b[39m\u001b[33mpair\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.trunk2sm_z(s_z)},\n\u001b[32m   1967\u001b[39m     true_aa,\n\u001b[32m   1968\u001b[39m     mask.float(),\n\u001b[32m   1969\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/transformers/models/esm/modeling_esmfold.py:1946\u001b[39m, in \u001b[36mEsmFoldingTrunk.forward.<locals>.trunk_iter\u001b[39m\u001b[34m(s, z, residx, mask)\u001b[39m\n\u001b[32m   1943\u001b[39m z = z + \u001b[38;5;28mself\u001b[39m.pairwise_positional_embedding(residx, mask=mask)\n\u001b[32m   1945\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m-> \u001b[39m\u001b[32m1946\u001b[39m     s, z = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidue_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m s, z\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/transformers/models/esm/modeling_esmfold.py:1243\u001b[39m, in \u001b[36mEsmFoldTriangularSelfAttentionBlock.forward\u001b[39m\u001b[34m(self, sequence_state, pairwise_state, mask, chunk_size, **_EsmFoldTriangularSelfAttentionBlock__kwargs)\u001b[39m\n\u001b[32m   1240\u001b[39m pairwise_state = pairwise_state + \u001b[38;5;28mself\u001b[39m.row_drop(\u001b[38;5;28mself\u001b[39m.tri_mul_out(pairwise_state, mask=tri_mask))\n\u001b[32m   1241\u001b[39m pairwise_state = pairwise_state + \u001b[38;5;28mself\u001b[39m.col_drop(\u001b[38;5;28mself\u001b[39m.tri_mul_in(pairwise_state, mask=tri_mask))\n\u001b[32m   1242\u001b[39m pairwise_state = pairwise_state + \u001b[38;5;28mself\u001b[39m.row_drop(\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtri_att_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairwise_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtri_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1244\u001b[39m )\n\u001b[32m   1245\u001b[39m pairwise_state = pairwise_state + \u001b[38;5;28mself\u001b[39m.col_drop(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m.tri_att_end(pairwise_state, mask=tri_mask, chunk_size=chunk_size)\n\u001b[32m   1247\u001b[39m )\n\u001b[32m   1249\u001b[39m \u001b[38;5;66;03m# MLP over pairs.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/transformers/models/esm/modeling_esmfold.py:601\u001b[39m, in \u001b[36mEsmFoldTriangleAttention.forward\u001b[39m\u001b[34m(self, x, mask, chunk_size, use_memory_efficient_kernel, use_lma, inplace_safe)\u001b[39m\n\u001b[32m    592\u001b[39m     x = \u001b[38;5;28mself\u001b[39m._chunk(\n\u001b[32m    593\u001b[39m         x,\n\u001b[32m    594\u001b[39m         biases,\n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m         inplace_safe=inplace_safe,\n\u001b[32m    599\u001b[39m     )\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_memory_efficient_kernel\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_memory_efficient_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lma\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_lma\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.starting:\n\u001b[32m    606\u001b[39m     x = x.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/transformers/models/esm/modeling_esmfold.py:488\u001b[39m, in \u001b[36mEsmFoldAttention.forward\u001b[39m\u001b[34m(self, q_x, kv_x, biases, use_memory_efficient_kernel, use_lma, lma_q_chunk_size, lma_kv_chunk_size, use_flash, flash_mask)\u001b[39m\n\u001b[32m    485\u001b[39m key = permute_final_dims(key, (\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m    487\u001b[39m \u001b[38;5;66;03m# [*, H, Q, K]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m biases:\n\u001b[32m    490\u001b[39m     output += b\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 59.25 GiB. GPU 0 has a total capacity of 79.22 GiB of which 44.74 GiB is free. Including non-PyTorch memory, this process has 34.46 GiB memory in use. Of the allocated memory 33.53 GiB is allocated by PyTorch, and 276.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define a desired batch size (modify as needed)\n",
    "batch_size = 4\n",
    "\n",
    "# Initialize your output dictionary with metadata keys;\n",
    "# keys from model output will be added on first iteration.\n",
    "output_dict = {\"protein_id\": [], \"sequence\": [], \"pdb\": []}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Process the DataFrame in batches\n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Prepare the list of sequences in the current batch\n",
    "        sequences = batch_df[\"sequence\"].tolist()\n",
    "        \n",
    "        # Tokenize the batch of sequences and move the tensor to GPU\n",
    "        tokenized_input = tokenizer(\n",
    "            sequences, \n",
    "            return_tensors=\"pt\", \n",
    "            add_special_tokens=False,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        )['input_ids'].cuda()\n",
    "\n",
    "        # Run the model on the batched input\n",
    "        output = model(tokenized_input)\n",
    "\n",
    "        # Initialize any additional keys from the model output during the first batch\n",
    "        if i == 0:\n",
    "            for k in output.keys():\n",
    "                output_dict[k] = []\n",
    "\n",
    "        # Loop over each key in the model output and process each sample in the batch\n",
    "        for k, value in output.items():\n",
    "            # Convert the output tensor to a NumPy array\n",
    "            output_np = value.cpu().numpy()\n",
    "            # Append each sample separately to keep alignment with metadata\n",
    "            for sample_idx in range(output_np.shape[0]):\n",
    "                output_dict[k].append(output_np[sample_idx])\n",
    "\n",
    "        # Process protein ids and sequences from the batch\n",
    "        output_dict[\"protein_id\"].extend(batch_df[\"id\"].tolist())\n",
    "        output_dict[\"sequence\"].extend(sequences)\n",
    "\n",
    "        # For each sample in the batch, generate the corresponding pdb string.\n",
    "        # Here we process them one by one to ensure compatibility with output_to_pdb.\n",
    "        for sample_idx in range(tokenized_input.size(0)):\n",
    "            # Prepare a dictionary with the output slice for this specific sample.\n",
    "            single_sample_output = {k: value[sample_idx:sample_idx+1] for k, value in output.items()}\n",
    "            pdb_str = model.output_to_pdb(single_sample_output)\n",
    "            output_dict[\"pdb\"].append(pdb_str)\n",
    "\n",
    "# Create a DataFrame from the completed dictionary.\n",
    "output_df = pd.DataFrame(output_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>pdb</th>\n",
       "      <th>frames</th>\n",
       "      <th>sidechain_frames</th>\n",
       "      <th>unnormalized_angles</th>\n",
       "      <th>angles</th>\n",
       "      <th>positions</th>\n",
       "      <th>states</th>\n",
       "      <th>s_s</th>\n",
       "      <th>...</th>\n",
       "      <th>residx_atom37_to_atom14</th>\n",
       "      <th>atom37_atom_exists</th>\n",
       "      <th>residue_index</th>\n",
       "      <th>lddt_head</th>\n",
       "      <th>plddt</th>\n",
       "      <th>ptm_logits</th>\n",
       "      <th>ptm</th>\n",
       "      <th>aligned_confidence_probs</th>\n",
       "      <th>predicted_aligned_error</th>\n",
       "      <th>max_predicted_aligned_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MGAGASAEEKHSRELEKKLKEDAEKDARTVKLLLLGAGESGKSTIV...</td>\n",
       "      <td>[PARENT N/A\\nATOM      1  N   MET A   1       ...</td>\n",
       "      <td>[[[[ 0.53268844  0.42589465  0.6979253  -0.218...</td>\n",
       "      <td>[[[[[[-6.9713593e-02  8.2730621e-01  5.5740875...</td>\n",
       "      <td>[[[[[-0.72785556 -0.68563384]\\n [-0.7659888  -...</td>\n",
       "      <td>[[[[[-0.7279038  -0.68567926]\\n [-0.766075   -...</td>\n",
       "      <td>[[[[[ 5.1296043 19.622414  69.8106   ]\\n [ 3.9...</td>\n",
       "      <td>[[[[ 2.22066760e-01 -2.29268894e-01 -4.8433545...</td>\n",
       "      <td>[[[-36.013134, -5.151828, 50.13264, -20.710196...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[[0, 1, 2, 4, 3, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[[[[[ -3.5443087   -1.6201594   12.064354   .....</td>\n",
       "      <td>[[[0.8376053, 0.86606765, 0.85308105, 0.789050...</td>\n",
       "      <td>[[[[31.894642  26.905233  21.444742  18.208603...</td>\n",
       "      <td>0.94865006</td>\n",
       "      <td>[[[[9.9319035e-01 6.7633195e-03 2.8754030e-05 ...</td>\n",
       "      <td>[[[0.25346506, 0.92572325, 2.2531812, 2.304195...</td>\n",
       "      <td>31.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  protein_id                                           sequence  \\\n",
       "0          1  MGAGASAEEKHSRELEKKLKEDAEKDARTVKLLLLGAGESGKSTIV...   \n",
       "\n",
       "                                                 pdb  \\\n",
       "0  [PARENT N/A\\nATOM      1  N   MET A   1       ...   \n",
       "\n",
       "                                              frames  \\\n",
       "0  [[[[ 0.53268844  0.42589465  0.6979253  -0.218...   \n",
       "\n",
       "                                    sidechain_frames  \\\n",
       "0  [[[[[[-6.9713593e-02  8.2730621e-01  5.5740875...   \n",
       "\n",
       "                                 unnormalized_angles  \\\n",
       "0  [[[[[-0.72785556 -0.68563384]\\n [-0.7659888  -...   \n",
       "\n",
       "                                              angles  \\\n",
       "0  [[[[[-0.7279038  -0.68567926]\\n [-0.766075   -...   \n",
       "\n",
       "                                           positions  \\\n",
       "0  [[[[[ 5.1296043 19.622414  69.8106   ]\\n [ 3.9...   \n",
       "\n",
       "                                              states  \\\n",
       "0  [[[[ 2.22066760e-01 -2.29268894e-01 -4.8433545...   \n",
       "\n",
       "                                                 s_s  ...  \\\n",
       "0  [[[-36.013134, -5.151828, 50.13264, -20.710196...  ...   \n",
       "\n",
       "                             residx_atom37_to_atom14  \\\n",
       "0  [[[0, 1, 2, 4, 3, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                  atom37_atom_exists  \\\n",
       "0  [[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                                       residue_index  \\\n",
       "0  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "\n",
       "                                           lddt_head  \\\n",
       "0  [[[[[ -3.5443087   -1.6201594   12.064354   .....   \n",
       "\n",
       "                                               plddt  \\\n",
       "0  [[[0.8376053, 0.86606765, 0.85308105, 0.789050...   \n",
       "\n",
       "                                          ptm_logits         ptm  \\\n",
       "0  [[[[31.894642  26.905233  21.444742  18.208603...  0.94865006   \n",
       "\n",
       "                            aligned_confidence_probs  \\\n",
       "0  [[[[9.9319035e-01 6.7633195e-03 2.8754030e-05 ...   \n",
       "\n",
       "                             predicted_aligned_error  \\\n",
       "0  [[[0.25346506, 0.92572325, 2.2531812, 2.304195...   \n",
       "\n",
       "  max_predicted_aligned_error  \n",
       "0                       31.75  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"structural_test.npy\", output_df.to_numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_loaded = np.load(\"structural_test.npy\", allow_pickle=True)\n",
    "output_df_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict_loaded = {k : [] for k in ['protein_id', 'sequence', 'pdb', 'frames', 'sidechain_frames',\n",
    "       'unnormalized_angles', 'angles', 'positions', 'states', 's_s', 's_z',\n",
    "       'distogram_logits', 'lm_logits', 'aatype', 'atom14_atom_exists',\n",
    "       'residx_atom14_to_atom37', 'residx_atom37_to_atom14',\n",
    "       'atom37_atom_exists', 'residue_index', 'lddt_head', 'plddt',\n",
    "       'ptm_logits', 'ptm', 'aligned_confidence_probs',\n",
    "       'predicted_aligned_error', 'max_predicted_aligned_error']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array(0.94865006, dtype=float32) (numpy-scalar) is not JSON serializable at the moment",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43moutput_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstructural_test.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/pandas/core/generic.py:2702\u001b[39m, in \u001b[36mNDFrame.to_json\u001b[39m\u001b[34m(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001b[39m\n\u001b[32m   2699\u001b[39m config.is_nonnegative_int(indent)\n\u001b[32m   2700\u001b[39m indent = indent \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2705\u001b[39m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2715\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2716\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2717\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/pandas/io/json/_json.py:210\u001b[39m, in \u001b[36mto_json\u001b[39m\u001b[34m(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mobj\u001b[39m\u001b[33m'\u001b[39m\u001b[33m should be a Series or a DataFrame\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    200\u001b[39m s = \u001b[43mwriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lines:\n\u001b[32m    213\u001b[39m     s = convert_to_line_delimits(s)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/compbio/lib/python3.13/site-packages/pandas/io/json/_json.py:263\u001b[39m, in \u001b[36mWriter.write\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    262\u001b[39m     iso_dates = \u001b[38;5;28mself\u001b[39m.date_format == \u001b[33m\"\u001b[39m\u001b[33miso\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mujson_dumps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj_to_write\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43miso_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43miso_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: array(0.94865006, dtype=float32) (numpy-scalar) is not JSON serializable at the moment"
     ]
    }
   ],
   "source": [
    "output_df.to_json(\"structural_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EsmForProteinFolding(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 2560, padding_idx=1)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 2560, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-35): 36 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "              (key): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "              (value): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (rotary_embeddings): RotaryEmbedding()\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=1440, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (esm_s_mlp): Sequential(\n",
      "    (0): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=2560, out_features=1024, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (embedding): Embedding(23, 1024, padding_idx=0)\n",
      "  (trunk): EsmFoldingTrunk(\n",
      "    (pairwise_positional_embedding): EsmFoldRelativePosition(\n",
      "      (embedding): Embedding(66, 128)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-47): 48 x EsmFoldTriangularSelfAttentionBlock(\n",
      "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (sequence_to_pair): EsmFoldSequenceToPair(\n",
      "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (pair_to_sequence): EsmFoldPairToSequence(\n",
      "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
      "        )\n",
      "        (seq_attention): EsmFoldSelfAttention(\n",
      "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (tri_mul_out): EsmFoldTriangleMultiplicativeUpdate(\n",
      "          (linear_a_p): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_a_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_b_p): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_b_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_z): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (layer_norm_in): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (layer_norm_out): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (tri_mul_in): EsmFoldTriangleMultiplicativeUpdate(\n",
      "          (linear_a_p): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_a_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_b_p): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_b_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (linear_z): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "          (layer_norm_in): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (layer_norm_out): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (tri_att_start): EsmFoldTriangleAttention(\n",
      "          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear): EsmFoldLinear(in_features=128, out_features=4, bias=False)\n",
      "          (mha): EsmFoldAttention(\n",
      "            (linear_q): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
      "            (linear_k): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
      "            (linear_v): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
      "            (linear_o): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "        (tri_att_end): EsmFoldTriangleAttention(\n",
      "          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear): EsmFoldLinear(in_features=128, out_features=4, bias=False)\n",
      "          (mha): EsmFoldAttention(\n",
      "            (linear_q): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
      "            (linear_k): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
      "            (linear_v): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
      "            (linear_o): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "        (mlp_seq): EsmFoldResidueMLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (2): ReLU()\n",
      "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (4): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp_pair): EsmFoldResidueMLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (2): ReLU()\n",
      "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (4): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (drop): Dropout(p=0, inplace=False)\n",
      "        (row_drop): EsmFoldDropout(\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (col_drop): EsmFoldDropout(\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (recycle_s_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (recycle_z_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (recycle_disto): Embedding(15, 128)\n",
      "    (structure_module): EsmFoldStructureModule(\n",
      "      (layer_norm_s): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm_z): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear_in): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
      "      (ipa): EsmFoldInvariantPointAttention(\n",
      "        (linear_q): EsmFoldLinear(in_features=384, out_features=192, bias=True)\n",
      "        (linear_kv): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
      "        (linear_q_points): EsmFoldLinear(in_features=384, out_features=144, bias=True)\n",
      "        (linear_kv_points): EsmFoldLinear(in_features=384, out_features=432, bias=True)\n",
      "        (linear_b): EsmFoldLinear(in_features=128, out_features=12, bias=True)\n",
      "        (linear_out): EsmFoldLinear(in_features=2112, out_features=384, bias=True)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "      )\n",
      "      (ipa_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm_ipa): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (transition): EsmFoldStructureModuleTransition(\n",
      "        (layers): ModuleList(\n",
      "          (0): EsmFoldStructureModuleTransitionLayer(\n",
      "            (linear_1): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
      "            (linear_2): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
      "            (linear_3): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (bb_update): EsmFoldBackboneUpdate(\n",
      "        (linear): EsmFoldLinear(in_features=384, out_features=6, bias=True)\n",
      "      )\n",
      "      (angle_resnet): EsmFoldAngleResnet(\n",
      "        (linear_in): EsmFoldLinear(in_features=384, out_features=128, bias=True)\n",
      "        (linear_initial): EsmFoldLinear(in_features=384, out_features=128, bias=True)\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x EsmFoldAngleResnetBlock(\n",
      "            (linear_1): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_2): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (linear_out): EsmFoldLinear(in_features=128, out_features=14, bias=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (trunk2sm_s): Linear(in_features=1024, out_features=384, bias=True)\n",
      "    (trunk2sm_z): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (distogram_head): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (ptm_head): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (lm_head): Linear(in_features=1024, out_features=23, bias=True)\n",
      "  (lddt_head): Sequential(\n",
      "    (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=384, out_features=128, bias=True)\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): Linear(in_features=128, out_features=1850, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
